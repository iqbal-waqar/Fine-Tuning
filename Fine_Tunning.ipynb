{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhpy1j35frEjgY1n62cMBV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iqbal-waqar/Fine-Tuning/blob/main/Fine_Tunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installation & Setup**"
      ],
      "metadata": {
        "id": "Rb6nFkVMEc4C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aD_uBAv8lOAS"
      },
      "outputs": [],
      "source": [
        "# Install Unsloth, a library that dramatically speeds up fine-tuning and inference for LLMs.\n",
        "!pip install unsloth # install unsloth\n",
        "\n",
        "\n",
        "# Force a clean reinstall of the latest Unsloth version from GitHub to ensure we have the most recent updates and bug fixes.\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries :\n",
        "\n",
        "# - Unsloth: For efficient model loading and training.\n",
        "# - torch: The core PyTorch library for tensor operations and GPU management.\n",
        "# - trl: Hugging Face's library for Transformer Reinforcement Learning (contains SFTTrainer).\n",
        "# - huggingface_hub: To securely log in to Hugging Face and access models/datasets.\n",
        "# - transformers: For the TrainingArguments to configure the training process.\n",
        "# - datasets: To load the medical dataset from the Hugging Face hub.\n",
        "# - wandb: For experiment tracking and logging training metrics.\n",
        "\n",
        "\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "from huggingface_hub import login\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import wandb"
      ],
      "metadata": {
        "id": "5pn9yXsPqNLa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely retrieve the Hugging Face API token stored in Colab's 'Secrets' manager.\n",
        "# This token is required to access gated models (like DeepSeek) and datasets\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_API_KEY')\n",
        "\n",
        "# Log in to Hugging Face using the API key.\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "uc5xWDMZqNQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that Colab has a GPU available and check which GPU it is.\n",
        "# This is crucial as Unsloth and PyTorch require a CUDA-enabled GPU for acceleration.\n",
        "\n",
        "\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "osR012IRqNTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Loading**"
      ],
      "metadata": {
        "id": "5dDfiTMlEkJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model we want to use and the configuration for loading it, we are using Deepseek R1 here.\n",
        "\n",
        "\n",
        "# The pre-trained model on Hugging Face Hub.\n",
        "model_name = \"DeepSeek-AI/DeepSeek-R1-Distill-Llama-8B\"\n",
        "\n",
        "\n",
        "# The maximum length of input sequences the model can handle.\n",
        "max_sequence_length = 2048\n",
        "\n",
        "# Let Unsloth automatically decide the best data type (e.g., bfloat16).\n",
        "dtype = None\n",
        "\n",
        "# Use 4-bit quantization to drastically reduce GPU memory usage.\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "# Load the model and its corresponding tokenizer with the specified settings.\n",
        "# The `token` parameter is needed if the model is gated (requires acceptance of terms).\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_sequence_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "XUqVlZaKqNWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Engineering & Initial Test**"
      ],
      "metadata": {
        "id": "mj2u-iSlEpAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a template to structure our prompts. This helps guide the model to generate answers in a specific format.\n",
        "# It includes placeholders `{}` for the question and the model's answer.\n",
        "\n",
        "\n",
        "prompt_style = \"\"\"\n",
        "Below is a task description along with additional context provided in the input section. Your goal is to provide a well-reasoned response that effectively addresses the request.\n",
        "\n",
        "Before crafting your answer, take a moment to carefully analyze the question. Develop a clear, step-by-step thought process to ensure your response is both logical and accurate.\n",
        "\n",
        "### Task:\n",
        "You are a medical expert specializing in clinical reasoning, diagnostics, and treatment planning. Answer the medical question below using your advanced knowledge.\n",
        "\n",
        "### Query:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0FvpzQFcqNY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our test medical question. We will use this to test the model before and after fine-tuning.\n",
        "\n",
        "\n",
        "question = \"\"\"A 68-year-old man with a history of atrial fibrillation on apixaban presents with sudden-onset right-sided\n",
        "hemiplegia and global aphasia, confirmed by MRI to have an acute left MCA infarction secondary to a cardioembolic clot.\n",
        "Following failed mechanical thrombectomy, his NIHSS score remains at 18. Given the known mechanism and his baseline\n",
        "anticoagulation, what is the most precise pathophysiological reason that makes intravenous thrombolysis with alteplase\n",
        "a potentially high-risk yet controversial consideration in this specific scenario?\"\"\"\n",
        "\n",
        "\n",
        "# Prepare the model for efficient text generation (inference).\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "\n",
        "# Format the prompt with our question and an empty string for the answer, then convert it into tokens (numbers) the model understands.\n",
        "# Move the tokens to the GPU for fast processing.\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "# Generate a response from the model. It will create new tokens based on the input.\n",
        "outputs = model.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "\n",
        "    # Tells the model to ignore padding tokens.\n",
        "    attention_mask = inputs.attention_mask,\n",
        "\n",
        "    # The maximum number of new tokens to generate.\n",
        "    max_new_tokens = 1200,\n",
        "\n",
        "    # Uses past key/values to speed up generation.\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Convert the generated token IDs back into human-readable text.\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "\n",
        "# Print the full generated text, including our original prompt.\n",
        "print(response)"
      ],
      "metadata": {
        "id": "acoQvoi3qNb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up the output by splitting the text and only printing the part after \"### Answer:\".\n",
        "\n",
        "\n",
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "Q6BiGsSoqNeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "b5CcZrq6Eugh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the medical dataset from the Hugging Face hub.\n",
        "# We are taking the first 600 examples from the English split for training.\n",
        "\n",
        "\n",
        "medical_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:600]\", trust_remote_code = True)"
      ],
      "metadata": {
        "id": "xhqXTShSxm2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the structure of one data example to understand its fields (Question, Complex_CoT, Response).\n",
        "\n",
        "medical_dataset[1]"
      ],
      "metadata": {
        "id": "hBJmFS_Exm_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tokenizer's \"End of Sequence\" token. This is crucial for telling the model where one training example ends and the next begins.\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating the text during training\n",
        "EOS_TOKEN"
      ],
      "metadata": {
        "id": "0UVh6p5qxnB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a more detailed prompt template for *training*. It includes separate placeholders for the question, the chain-of-thought (reasoning), and the final answer.\n",
        "\n",
        "\n",
        "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "\n",
        "### Instruction:\n",
        "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\n",
        "Please answer the following medical question.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\n",
        "{}\n",
        "\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "6g2lEKyixnEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess each example in the dataset into the format required for training.\n",
        "\n",
        "def preprocess_input_data(examples):\n",
        "  inputs = examples[\"Question\"]\n",
        "\n",
        "# The step-by-step reasoning.\n",
        "  cots = examples[\"Complex_CoT\"]\n",
        "# The final answer.\n",
        "  outputs = examples[\"Response\"]\n",
        "\n",
        "# This list will hold all our formatted training texts.\n",
        "  texts = []\n",
        "\n",
        "# For each example, combine the question, reasoning, and answer into a single string using our template.\n",
        "  for input, cot, output in zip(inputs, cots, outputs):\n",
        "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN # Append the EOS token.\n",
        "    texts.append(text)\n",
        "\n",
        "\n",
        "# Return a dictionary with the key \"texts\".\n",
        "  return {\n",
        "      \"texts\" : texts,\n",
        "  }"
      ],
      "metadata": {
        "id": "lwb6XhBLxnHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing function to the entire dataset. This creates a new dataset with a \"texts\" column.\n",
        "\n",
        "finetune_dataset = medical_dataset.map(preprocess_input_data, batched = True)"
      ],
      "metadata": {
        "id": "F_h5Tf-OxnJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check one of the formatted training examples to ensure it looks correct.\n",
        "\n",
        "finetune_dataset[\"texts\"][0]"
      ],
      "metadata": {
        "id": "pLRhfYXlxnMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Preparation for Fine-Tuning (Low Rank Adaptation --LoRA)**"
      ],
      "metadata": {
        "id": "91E8asxCE2nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA (Low-Rank Adaptation) to the model. This is a parameter-efficient fine-tuning technique.\n",
        "# Instead of training all 8 billion parameters, we only train a small set of adapters, making it much faster and using less memory.\n",
        "\n",
        "\n",
        "model_lora = FastLanguageModel.get_peft_model(\n",
        "    model=model,\n",
        "\n",
        "# The rank of the LoRA matrices. A lower rank means fewer parameters to train.\n",
        "    r=16,\n",
        "\n",
        "# The names of the model's layers we want to attach LoRA adapters to.\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "\n",
        "# A scaling factor for LoRA.\n",
        "    lora_alpha=16,\n",
        "\n",
        "# Dropout probability for LoRA layers (0 for no dropout).\n",
        "    lora_dropout=0,\n",
        "\n",
        "# Do not train bias parameters.\n",
        "    bias=\"none\",\n",
        "\n",
        "# Saves memory by trading compute for memory.\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "\n",
        "# Seed for reproducibility.\n",
        "    random_state=3047,\n",
        "\n",
        "# Don't use RS-LoRA variant.\n",
        "    use_rslora=False,\n",
        "\n",
        "# Don't use LoftQ initialization.\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "id": "MvLSXOalxnPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is used by the SFTTrainer to format the training data on-the-fly during training.\n",
        "\n",
        "def formatting_func(examples):\n",
        "    texts = []\n",
        "    for input, cot, output in zip(examples[\"Question\"], examples[\"Complex_CoT\"], examples[\"Response\"]):\n",
        "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "QJ2BkTQ_6Th0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a potential fix for a known issue with Unsloth to ensure the model's `generate` method works correctly after applying LoRA.\n",
        "\n",
        "if hasattr(model, '_unwrapped_old_generate'):\n",
        "    del model._unwrapped_old_generate"
      ],
      "metadata": {
        "id": "1wKjGklSxnRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Setup & Execution**"
      ],
      "metadata": {
        "id": "6dW74TW8Fn0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer, which handles the entire training loop.\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model_lora, # The LoRA-adapted model we want to train.\n",
        "\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        "    train_dataset=finetune_dataset, # Our preprocessed dataset.\n",
        "\n",
        "    max_seq_length=max_sequence_length, # Truncate/pad sequences to this length.\n",
        "\n",
        "    dataset_num_proc=1, # Number of processes for dataset preprocessing.\n",
        "\n",
        "    formatting_func=formatting_func, # The function that tells the trainer how to format each data example.\n",
        "\n",
        "    # Define training args\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "E9dueLYi4uWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely retrieve the Weights & Biases API key for experiment tracking.\n",
        "\n",
        "from google.colab import userdata\n",
        "wnb_token = userdata.get(\"WANDB_API_KEY\")"
      ],
      "metadata": {
        "id": "YYz_LW3E4uZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to W&B and initialize a new run to track our training metrics (loss, learning rate, etc.).\n",
        "\n",
        "wandb.login(key=wnb_token) # import wandb\n",
        "run = wandb.init(\n",
        "    project='Fine-tune-DeepSeek-R1-on-Medical-CoT-Dataset',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "id": "I7gmHibx4ubP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the fine-tuning process! This is where the model actually learns from the medical dataset.\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "mDkK8Nrq4udg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finalize the W&B run and upload all remaining data.\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "0PwCZ5LZ4ufn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the same initial question again, but now using the fine-tuned model (model_lora).\n",
        "\n",
        "question = \"\"\"A 68-year-old man with a history of atrial fibrillation on apixaban presents with sudden-onset right-sided\n",
        "hemiplegia and global aphasia, confirmed by MRI to have an acute left MCA infarction secondary to a cardioembolic clot.\n",
        "Following failed mechanical thrombectomy, his NIHSS score remains at 18. Given the known mechanism and his baseline\n",
        "anticoagulation, what is the most precise pathophysiological reason that makes intravenous thrombolysis with alteplase\n",
        "a potentially high-risk yet controversial consideration in this specific scenario?\"\"\"\n",
        "\n",
        "\n",
        "# Prepare the fine-tuned model for inference.\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model_lora.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "sF5JmlRK4ukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "T_s48O5W4ums"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue,\n",
        "              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative,\n",
        "              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium.\n",
        "              What is the most likely predisposing factor for this patient's condition?\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model_lora)\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "outputs = model_lora.generate (\n",
        "    input_ids = inputs.input_ids,\n",
        "    attention_mask = inputs.attention_mask,\n",
        "    max_new_tokens = 1200,\n",
        "    use_cache = True\n",
        ")\n",
        "\n",
        "# Decode the response tokens back to text\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(response[0].split(\"### Answer:\")[1])"
      ],
      "metadata": {
        "id": "b_zeYTOiA7qH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}